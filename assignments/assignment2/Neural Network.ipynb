{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cb59275d7c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO Now implement backward pass and aggregate all of the params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcheck_model_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Projects/PythonProjects/dlcourse_ai/assignments/assignment2/gradient_check.py\u001b[0m in \u001b[0;36mcheck_model_gradient\u001b[0;34m(model, X, y, delta, tol)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhelper_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/PythonProjects/dlcourse_ai/assignments/assignment2/gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[0;34m(f, x, delta, tol)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0manalytic_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalytic_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mnumeric_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric_grad_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# We will go through every dimension of x and compute numeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/PythonProjects/dlcourse_ai/assignments/assignment2/gradient_check.py\u001b[0m in \u001b[0;36mnumeric_grad_array\u001b[0;34m(f, x, h)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx_plus_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx_minus_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_plus_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_minus_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miternext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/PythonProjects/dlcourse_ai/assignments/assignment2/gradient_check.py\u001b[0m in \u001b[0;36mhelper_func\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhelper_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/PythonProjects/dlcourse_ai/assignments/assignment2/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0md_out2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0md_relu_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0md_out1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_relu_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/PythonProjects/dlcourse_ai/assignments/assignment2/layers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, d_out)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \"\"\"\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0md_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.505085, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: 2.505085, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: 2.505085, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: 2.505085, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: 2.505085, Train accuracy: 0.066333, val accuracy: 0.049000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2, num_epochs=5)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x103a78590>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhail/Documents/Projects/PythonProjects/dlcourse_ai/assignments/assignment2/layers.py:21: RuntimeWarning: invalid value encountered in true_divide\n",
      "  res = pred_exp / np.sum(pred_exp, axis=1, keepdims=True)\n",
      "/Users/mikhail/Documents/Projects/PythonProjects/dlcourse_ai/assignments/assignment2/layers.py:131: RuntimeWarning: invalid value encountered in greater\n",
      "  d_result = (self.X > 0) * d_out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99, num_epochs=5)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
